---
layout: post
title:  "AI Safety is Clearly Bullshit"
date:   2020-06-05 00:00:00 +0000
categories: RL  
---

# AI Safety Bullshit

Some people are worried that they'll accidentally make a superintelligence artificial intelligence that will turn them into paperclips. They spend a lot of time thinking critically and seriously about how to avoid being turned into paperclips, and what else a wayward AI might unexpectedly try to turn them into. The thought experiment underlying these fears goes something like this: if someone makes a smart AI, roughly equivalent to the intelligence of its makers, it will immediately get super-smart. This is because if an AI is as smart as its creators are, it should be at least as good of an AI engineer and can engage in a rapidly compounding spiral of self-improvement. AI researchers and science-fiction aficionados alike have been conditioned by years of things going wrong by going exactly right in an unexpected way (in reinforcement learning we call this reward hacking). Consequently it's commonly believed that AI systems need very carefully designed objectives to prevent them from solving them in an unwanted way. But when we spend all our professional attention on the consequences of a rogue super-intelligent AI, we're missing the point. Focusing on an outcome that, while potentially world-upending, has an unknown but low probability of occurence on the timescale of decades as a mental exercise doesn't hurt anyone. Except when it detracts from contending with clear and present abuses of technology broadly related to AI systems. 

And we do have a multitude of technological abuses leveraging "AI" technology. They collate your shopping and movemement patterns. They scrape and collate your photos from the web for stalkers, police, and stalker-police to abuse. They deny your loan applications, aggravate mental health stressors, and twist your political systems in a way that professional propagandists of the last century could only dream of. The list is too long to discuss each element in detail, and it's incomplete to boot. The objective of this essay is to get you, the reader with professional proximity to automatable tech and machine learning to consider spending some time thinking about these very ground issues of technologically-enhanced suppression of human freedom, and you, the reader who doesn't know much about AI or technology, to get an idea for whats in use today so you can reason thoughtfully about the morality of the topic. 




We can't even cultivate value-alignment in our human institutions. 

eff.org
algorithmic justice leage
aclu.org   
